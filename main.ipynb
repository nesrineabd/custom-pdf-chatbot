{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_V-bI5VAMHr"
      },
      "source": [
        "## A project aiming to develop a personalized chatbot that will allow us to ask questions to a specific set of data that we provide.\n",
        "## It leverages Langchain as a model, OpenAI for embeddings generation, and FAISS library for efficient indexing and storage of text embeddings, all this based on a provided database in form of a PDF file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9WSlGWJ3e1k"
      },
      "source": [
        "### First set up the environement, download the needed libraries and set the OpenAI api key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9kYNkrsAPB2"
      },
      "outputs": [],
      "source": [
        "# Download the needed libraries\n",
        "\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OiNHrtn1Fr8e"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "D0Gff_O-HtyC"
      },
      "outputs": [],
      "source": [
        "# Get our API key from Openai, an account is needed.\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cnqG3E8RKbsz"
      },
      "outputs": [],
      "source": [
        "# Read in the pdf file\n",
        "\n",
        "\n",
        "pdf_reader = PdfReader(\"/content/BIG-DATA-COURS.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QjKqjFllM7mt"
      },
      "outputs": [],
      "source": [
        "# Read data from the file and put them into a string variable called text\n",
        "\n",
        "\n",
        "text = ''\n",
        "for i, page in enumerate(pdf_reader.pages):\n",
        "  tmp_text = page.extract_text()\n",
        "  if tmp_text:\n",
        "    text += tmp_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MjmlrQpvPr8Z"
      },
      "outputs": [],
      "source": [
        "# Chunk the Documents, the chunks are the building blocs for our LLM\n",
        "# It go from a simple letter 'g' to a complete word 'data'\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size = 512,\n",
        "  chunk_overlap  = 32,\n",
        "  length_function = len,\n",
        "  )\n",
        "texts = text_splitter.split_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6IE-6WmTTdR6"
      },
      "outputs": [],
      "source": [
        "# Initialize the embeddings from the OpenAI library\n",
        "# It will allow us to give a sementic to our data (contextualization)\n",
        "# we will use them to convert the chunks of text into vectors\n",
        "\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# to finally get a Vector Database (our knowledge base) using the FAISS library and the OpenAI embeddings\n",
        "\n",
        "\n",
        "docsearch = FAISS.from_texts(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2JYTFUY1TiGp"
      },
      "outputs": [],
      "source": [
        "# qa_chain is used to connect our similarity search to the promptsâ€“user input\n",
        "# it can be used for more complex tasks\n",
        "\n",
        "# The stuff parameter in our qa_chain enables us to build applications like\n",
        "# this, where documents are small and only a few are passed in for most calls\n",
        "\n",
        "\n",
        "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "ffHhx0HoVlsx",
        "outputId": "9cbd5c83-9a3b-4ba6-dc8f-99e4117bdf64"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Big data refers to a large volume of data that is characterized by its massive size, variety of formats and structures, and the need for fast processing. It is a complex type of data that requires advanced technologies and algorithms for storage and analysis. It is often defined using the concept of the 3Vs: velocity, variety, and volume.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Finally, we get to ask our PDF questions\n",
        "\n",
        "query = \"what is big data ?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "hPhOal7zARem",
        "outputId": "672a82e6-afc2-485e-d40d-019f40a877c8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This course is useful for engineers working with big data, as it covers topics such as Hadoop, MapReduce, and Spark, which are commonly used in big data technologies.'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"for which engineer profile is this course useful ?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A359ZsJqAVz-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
